# Job-Scraping & KI-Bewerbungsassistent ğŸ¤–ğŸ’¼

Eine Web-Anwendung, die automatisch Praktikumsstellen auf StepStone und Xing durchsucht, KI-gestÃ¼tztes Matching durchfÃ¼hrt, 
individuelle Anschreiben generiert und alle Ergebnisse als sortierte PDF-Zusammenfassung ausliefert.

---

## ğŸŒŸ Features

- **Multi-Platform Scraping**  
  â€“ StepStone & Xing  
- **KI-gestÃ¼tzte Analyse**  
  â€“ Job-Match-Rating (Skala 1â€“10)  
  â€“ Personalisiertes Anschreiben mit Google Gemini  
  â€“ Formatierung unstrukturierter Job-Beschreibungen  
- **PDF-Management**  
  â€“ Pro Stelle automatisch generierte PDF  
  â€“ ZusammenfÃ¼hrung und Sortierung nach Rating  
- **Profileingabe im Frontend**  
  â€“ Jobkriterien, Studium, Interessen, Skills  
  â€“ PDF Upload & Text-Extraktion im Client
- **Custom-Skills**  
  â€“ Vordefinierte Basis-Skills plus eigene Skills  
  â€“ Persistenz durch speicherung im LocalStorage  

---

## ğŸ—ï¸ Technologie-Stack

**Backend (Python 3.9+)**  
- Flask (Bereitstellung der REST-API-Endpoints & Auslieferung des statischen Frontend-Builds)
- Requests + BeautifulSoup (Versenden von HTTP-Anfragen)
- Selenium + webdriver-manager (Headless-Browser-Automatisierung fÃ¼r dynamische / Lazy-Load-Seiten)  
- BeautifulSoup (Parsen von HTML fÃ¼r das Web-Scraping)
- google-generativeai (Client-Bibliothek zur Anbindung an Google Gemini AI-Modelle fÃ¼r Text-Generierung)  
- xhtml2pdf & PyPDF2 (Konvertierung von HTML/Markdown in PDF & ZusammenfÃ¼hren mehrerer PDF-Dateie)  

**Frontend (React 18+)**  
- React, React-DOM, React-Hook-Form (Erstellen der UI-Komponenten, DOM-Rendering & Formular-Validierung)
- PDF.js (Parsen und Extrahieren von Text aus hochgeladenen PDF-Dokumenten im Browser)
- LocalStorage (Persistenz und Verwaltung der benutzerdefinierten Skills im Client)

---

## ğŸ“ Projektstruktur

```plaintext
infosys_done/
â””â”€â”€ projekt/
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ ai/
    â”‚   â”‚   â”œâ”€â”€ prompts/
    â”‚   â”‚   â”‚   â”œâ”€â”€ cover_letter_generation.txt
    â”‚   â”‚   â”‚   â”œâ”€â”€ job_description_formatting.txt
    â”‚   â”‚   â”‚   â””â”€â”€ job_rating.txt
    â”‚   â”‚   â”œâ”€â”€ gemini_client.py
    â”‚   â”‚   â”œâ”€â”€ prompt_manager.py
    â”‚   â”‚   â””â”€â”€ text_processor.py
    â”‚   â”œâ”€â”€ core/
    â”‚   â”‚   â”œâ”€â”€ api_key.txt
    â”‚   â”‚   â”œâ”€â”€ config.py
    â”‚   â”‚   â””â”€â”€ models.py
    â”‚   â”œâ”€â”€ scrapers/
    â”‚   â”‚   â”œâ”€â”€ request_base_scraper.py
    â”‚   â”‚   â”œâ”€â”€ selenium_base_scraper.py
    â”‚   â”‚   â”œâ”€â”€ stepstone_scraper.py
    â”‚   â”‚   â””â”€â”€ xing_scraper.py
    â”‚   â”œâ”€â”€ temp_pdfs/
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ html_parser.py
    â”‚       â””â”€â”€ pdf_utils.py
    â”œâ”€â”€ frontend/
    â”‚   â”œâ”€â”€ build/
    â”‚   â”œâ”€â”€ node_modules/
    â”‚   â”œâ”€â”€ public/
    â”‚   â””â”€â”€ src/
    â”‚       â”œâ”€â”€ components/
    â”‚       â”œâ”€â”€ constants/
    â”‚       â”œâ”€â”€ hooks/
    â”‚       â””â”€â”€ services/
    â”œâ”€â”€ main.py
    â””â”€â”€ requirements.txt
â””â”€â”€ tests/
```

---

## ğŸš€ Motivation

Aktuell verbringen Studierende, wie meine Kommilitonen, viel Zeit mit der Suche nach einer passenden Praxissemesterstelle. Diese zeitaufwÃ¤ndige Suche, kombiniert mit dem Aufwand fÃ¼r individuelle Bewerbungen neben dem Studium und mÃ¶glichen Minijobs oder WerkstudententÃ¤tigkeiten, gestaltet sich nahezu unmÃ¶glich.

Mein Tool automatisiert:

1. Relevante Angebote identifizieren: Aktuell fokussiert auf Praktika und Werkstudentenjobs.
2. Profil-Matching: Abgleich zwischen Stellenbeschreibungen und Bewerberprofilen, um die wichtigsten Ãœbereinstimmungen herauszuarbeiten.
3. Erstellung individueller Anschreiben: Generierung passgenauer Anschreiben auf Basis von vorliegenden Informationen, wobei spezifische Details zu frÃ¼heren Anstellungen oder FÃ¤higkeiten aus den PDfs integriert werden.
4. Zusammenfassung der Ergebnisse: Aufbereitung aller relevanten Informationen in einer PDF-Zusammenfassung. Hierbei werden die Ergebnisse fÃ¼r jeden Job, der einen Mindestschwellenwert erreicht hat, einzeln dargestellt und anschlieÃŸend in einer finalen PDF-Ausgabe fÃ¼r das Frontend zusammengefasst.  

Durch diese Automatisierung bleibt mehr Zeit fÃ¼r fachliche Inhalte und die persÃ¶nliche Vorbereitung.

---

## âš ï¸ Probleme & LÃ¶sungsansatz

- **Dynamische Seiten und Lazy-Loading (Xing)**  
    AnfÃ¤nglich traten Probleme bei der Extraktion aller relevanten Informationen von der Xing-Webseite auf. Im Gegensatz zu StepStone basiert Xing auf JavaScript. Dies erschwert die Informationsverarbeitung durch Lazy-Loading und dient zudem als Mechanismus gegen Web-Scraping. Daher war es notwendig, eine geeignete LÃ¶sung fÃ¼r diese Herausforderung zu entwickeln.
    <br>
    â†’ Einsatz von Headless-Selenium zur serverseitigen Wiedergabe mit kontrollierten Scroll-Schleifen (bei 25 Anzeigen pro Lazy-Load-Schritt: Anzahl der Anzeigen / 25 = benÃ¶tigte Scroll-VorgÃ¤nge).

---

## ğŸ”„ Ablauf (End-to-End)

1. **Formular-Eingabe (Frontend)**  
   â€“ Erfassung von Jobkriterien, Studieninformationen, Interessen, FÃ¤higkeiten sowie hochgeladenen PDF-Dokumenten.
2. **PDF-Text-Extraktion**  
   â€“ DurchfÃ¼hrung der Text-Extraktion aus PDF-Dokumenten im Client mittels PDF.js.
3. **API-Request**  
   â€“ Versand der gesammelten Daten Ã¼ber einen POST-Request an /api/create_job.
4. **Session-Verzeichnis**  
   â€“ Anlegen eines temporÃ¤ren Verzeichnisses, das zeit- und parameterbasiert benannt ist, zur Speicherung von PDF-Dateien wÃ¤hrend der Sitzung.
5. **Scraping**  
   - **StepStone**  
     â€¢ HTTP-Client mit Requests â†’ HTML abrufen â†’ Ermittlung der Gesamtzahl der Anzeigen â†’ Extraktion der relevanten Stellen-Links â†’ Parsing der Detailinformationen zu jeder Stelle
   - **Xing**  
     â€¢ Headless-Chrome via Selenium â†’ Seite laden â†’ Ermittlung der erforderlichen Anzahl von Scroll-VorgÃ¤ngen â†’ DurchfÃ¼hrung von "Scroll-to-Bottom"-Aktionen â†’ Extraktion der relevanten Stellen-Links â†’ Parsing der Detailinformationen zu jeder Stelle
6. **KI-Pipeline**  
   â€“ Jede Stelle: nach  Praktikum/Werkstudenten gefiltert (SchlÃ¼sselbegriffe) â†’ Job-Beschreibung formatieren â†’ Rating abfragen â†’ Anschreiben generieren, wenn das Rating (rating â‰¥ cover_letter_min_rating_premium oder rating â‰¥ cover_letter_min_rating)
7. **PDF-Erzeugung & Merge**  
   â€“ Markdown-Teile (Rating, Titel, Beschreibung, Anschreiben) â†’ xhtml2pdf â†’ einzelne PDFs â†’  
     sortieren nach Rating â†’ ZusammenfÃ¼hren mit PyPDF2 â†’ `summary.pdf`.  
8. **Download**  
   â€“ `summary.pdf` als Antwort an den Client.

---
## ğŸ”® Verbesserungen & Ausblick

- **Parallelisierung des Scrapings**  
  â€¢ Einsatz von `asyncio` + `aiohttp` (oder alternativ Python-`multiprocessing`), um
    -  die Extraktion der Job-URLs gleichzeitig zu bearbeiten
  â†’ VerkÃ¼rzt die Wartezeiten und nutzt Systemressourcen effizienter.

- **Multithreading fÃ¼r KI-Anfragen**  
  â€¢ Das Rating und die Anschreiben-Generierung via Google Gemini API dominieren die Laufzeit.  
  â€¢ Durch einen Thread-Pool kÃ¶nnen mehrere Anfragen parallel abgewickelt werden
  â†’ Reduziert insgesamt die Antwortzeiten drastisch

- **Manuelle Nachbearbeitung im Frontend**  
  â€¢ Nach dem KI-Entwurf wird ein eingebetteter Editor (Markdown oder Rich-Text) bereitgestellt 
  â€¢ Nutzer kÃ¶nnen den Anschreiben-Text selbst anpassen, Layout Ã¤ndern oder Abschnitte ergÃ¤nzen
  â†’ Mehr Kontrolle und Individualisierung vor dem PDF-Export

- **Persistente Ergebnis-Speicherung**  
  â€¢ Statt nur temporÃ¤rer Dateien im `temp_pdfs`-Verzeichnis:  
    - Speichern aller generierten PDFs, Ratings, verwendeter Modelle und Prompts  
    - in einer Datenbank (z. B. PostgreSQL oder MongoDB)
  â€¢ ErmÃ¶glicht Analytics und kontinuierliches Prompt-Tuning auf Basis realer Nutzerdaten

Durch diese Erweiterungen wird das System
- spÃ¼rbar schneller in der Verarbeitung  
- und bietet den Anwendern mehr InteraktivitÃ¤t sowie eine lernende Grundlage zur weiteren Verbesserung der Anschreiben-QualitÃ¤t

---
## âš™ï¸ Installation

### 1. Repository klonen  
```bash
git clone <URL>
cd infosys_done/projekt
```

### 2. Backend einrichten  
```bash
pip install -r requirements.txt
```

### 3. Frontend einrichten  
```bash
cd frontend
npm install
npm run build
```

---

## ğŸ”§ Konfiguration

1. **API-Key**  
   â€“ Datei `backend/core/api_key.txt` mit Ihrem Google Gemini-API-SchlÃ¼ssel fÃ¼llen.  

2. **Pfade**  
   â€“ In `backend/core/config.py` unter `PathConfig` Pfade zu Frontend-Build, Temp-PDFs, Prompts, Logs anpassen.  

3. **AI-Modelle & Parameter**  
   â€“ In `backend/core/config.py` unter `AIConfig` Modelle (`AIModel`-Enum) und Temperaturen einstellen.  

4. **Scraping-Parameter**  
   â€“ In `backend/core/config.py` unter `ScrapingConfig` Delays, Timeouts, Retry-Strategien, Selenium-Defaults justieren.

5. **Filter von Jobs durch Jobkeys (Praktikum, Werkstudent, ...) erweitern**  
   â€“ In `backend/core/models.py` unter `JobDetailsScraped` in der `__post_init__(self)` Methode.
6. **Config Allgemein**
   - In der Datei backend/core/config.py sind sÃ¤mtliche globalen Parameter definiert. Dies umfasst beispielsweise die Anzahl der Jobs in der Zusammenfassung (summary) sowie die CSS-Selektoren fÃ¼r das HTML-Parsing. Falls AnpassungswÃ¼nsche bestehen, bietet die Datei backend/core/config.py Ã¼blicherweise alle notwendigen Elemente fÃ¼r eine Individualisierung.

---

## â–¶ï¸ Verwendung

```bash
 cd projekt
python -m projekt.main
```

1. Browser Ã¶ffnen: `http://localhost:5000`  
2. Formular ausfÃ¼llen (Jobsuche, Profil, PDF-Upload).  
3. Auf **â€Bewerbung sendenâ€œ** klicken.  
4. Automatisch generierte und zusammengefÃ¼hrte `summary.pdf` herunterladen.

---

## ğŸ‘¤ Ãœber mich
Ich bin nach der Abgabe des Projekts hochmotiviert, weiter daran zu arbeiten, wie man am Umfang des Projekts erkennen kann. Lustigerweise habe ich meinen Praxissemesterjob mit meinem Prototypen gefunden. Haben Sie Tipps, wie ich meine Anwendung erfolgreich 'an den Markt bringen' â€“ besser gesagt, den Leuten zur VerfÃ¼gung stellen â€“ kann, ohne potenzielle Probleme mit den jeweiligen Jobseiten zu bekommen? Dabei sind mir Dinge wie GitHub und Subreddits bereits ein Begriff. Mit freundlichen GrÃ¼ÃŸen
**Hakon Wahl**  
- Student der Wirtschaftsinformatik â€“ Informationstechnologie, HM MÃ¼nchen  
- GitHub: [@hakon-username](https://github.com/hakon-username)  
- E-Mail: hakon.wahl@gmail.com  

---

*Viel Erfolg beim Ausprobieren!*